{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1o_ChYMpLTeTsUoiJe2D_TPIZYcvSQx_c",
      "authorship_tag": "ABX9TyMQ+admhr1y2cvyvrr00+0v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgr1118/Sentence-Embedding-Using-Korean-Corpora/blob/main/03_%ED%95%9C%EA%B5%AD%EC%96%B4_%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한국어 전처리\n",
        "- 3장에서는 임베딩 학습을 위한 한국어 데이터 전처리 과정을 다룬다. 웹 문서나 json 같은 형태의 파일을 텍스트 파일로 바꾸고 여기에 형태소 분석을 실시하는 방법을 설명한다. 형태소 분석 방법에는 국어학 전문가들이 태깅한 데이터로 학습된 모델로 분석하는 지도 학습 기법, 우리가 가진 말뭉치의 패텅을 학습한 모델을 적용하는 비지도 학습 기법 등이 있다.\n",
        "\n",
        "## 3.1 데이터 확보\n",
        "- 임베딩 학습용 말뭉치는 직접 만들거나, 웹 스크래핑을 하여 모을 수 있다. 이 교재에서는 이미 공개돼있는 말뭉치 데이터를 활용\n",
        "\n",
        "- 임베딩에 자연어 의미를 함축하는 비법은 자연어의 통계적 패턴 정보를 통째로 임베딩에 넣는 것이다. 자연어의 의미는 해당 언어 화자들이 실제 사용하는 일상 언어에서 드러나기 때문이다. 임베딩을 만들 때 쓰는 통계 정보는 아래 3가지 있다.\n",
        "\n",
        "### 3.1.1 한국어 위키백과\n",
        "- 한국어 위키백과 raw data를 사용하려면 도커 컨테이너 bash shell을 사용하거나 pythn 상에서 코드로 불러 올 수 있다.\n",
        "\n",
        "코드 3-1 bash 한국어 위키백과 다운로드\n",
        "- git pull origin master\n",
        "- bash preprocess.sh dump-raw-wiki\n",
        "\n"
      ],
      "metadata": {
        "id": "KcPzAumlHIkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/MyDrive/Book/Sentence Embedding Using Korean Corpora/data'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Tno1ZiPcU_T",
        "outputId": "10fcac73-5721-4ce9-b755-8b30484d5e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Book/Sentence Embedding Using Korean Corpora/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-1 한국어 위키백과 다운로드\n",
        "!pip install wikiextractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXblyuFhwdhp",
        "outputId": "cb7471ae-0dac-4ff3-e9fb-1488a74f528d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wikiextractor\n",
            "  Downloading wikiextractor-3.0.6-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wikiextractor\n",
            "Successfully installed wikiextractor-3.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2\n",
        "# kowiki-latest-pages-articles.xml.bz2.1이 다운되는 현상이 발생하여 kowiki-latest-pages-articles.xml.bz2를 직접적으로 다운로드하여 사용"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OHGFaqtw99m",
        "outputId": "51e62838-643a-4795-d82f-de31dcce657c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-18 06:45:10--  https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.142, 2620:0:861:2:208:80:154:142\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 889750893 (849M) [application/octet-stream]\n",
            "Saving to: ‘kowiki-latest-pages-articles.xml.bz2.1’\n",
            "\n",
            "kowiki-latest-pages 100%[===================>] 848.53M  4.17MB/s    in 3m 22s  \n",
            "\n",
            "2023-03-18 06:48:32 (4.21 MB/s) - ‘kowiki-latest-pages-articles.xml.bz2.1’ saved [889750893/889750893]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위키익스트랙터를 사용하여 위키피디아 덤프를 파싱\n",
        "!python -m wikiextractor.WikiExtractor kowiki-latest-pages-articles.xml.bz2"
      ],
      "metadata": {
        "id": "FvxX4MCVxAi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQbwBcOIdMcM",
        "outputId": "afd4066e-973f-4b6e-c422-2fb85137b1cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mimages\u001b[0m/                                    LICENSE\n",
            "install_mecab-ko_on_colab190912.sh         README.md\n",
            "install_mecab-ko_on_colab_light_220429.sh  \u001b[01;34mtext\u001b[0m/\n",
            "kowiki-latest-pages-articles.xml.bz2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-2 한국어 위키백과 전처리\n",
        "import os\n",
        "import re\n",
        "\n",
        "os.listdir('text')"
      ],
      "metadata": {
        "id": "EH3LdnTzgfWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da49b92b-db43-4c20-d824-c77d177e8e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AA', 'AB', 'AC', 'AD', 'AE', 'AF', 'AG', 'AH', 'AI']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 위키백과 전처리\n",
        "\n",
        "from gensim.corpora import WikiCorpus, Dictionary\n",
        "from gensim.utils import to_unicode\n",
        "from gensim.corpora.wikicorpus import tokenize\n",
        "\n",
        "in_f = '/content/drive/MyDrive/Book/Sentence Embedding Using Korean Corpora/data/Mecab-ko-for-Google-Colab/kowiki-latest-pages-articles.xml.bz2'\n",
        "out_f = '/content/drive/MyDrive/Book/Sentence Embedding Using Korean Corpora/data/Mecab-ko-for-Google-Colab/precessed_wiki_ko.txt'\n",
        "output = open(out_f, 'w')\n",
        "wiki = WikiCorpus(in_f, tokenizer_func = tokenize, dictionary=Dictionary())\n",
        "i = 0\n",
        "for text in wiki.get_texts():\n",
        "    output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n')\n",
        "    i = i + 1\n",
        "    if (i % 10000 == 0):\n",
        "        print('Processed ' + str(i) + ' article')\n",
        "output.close()\n",
        "print('Processing complete!')"
      ],
      "metadata": {
        "id": "Bo8LY53voUxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 본문 <text> 태그 안에서 필요한 내용을 추출하는 과정이 필요하다. 여기서 특수문자, 목차, 이메일 주소 등 불필요한 문자열 제거가 필요하다.\n",
        "\n",
        "- 코드 3-2에서 tokenize 함수를 코드 3-4와 같이 수정하여 불 필요한 정보를 제거할 것이다."
      ],
      "metadata": {
        "id": "8OHlI82EHM8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-4 사용자 정의, 한국어 위키 토크나이저\n",
        "\n",
        "import re\n",
        "from gensim.utils import to_unicode\n",
        "\n",
        "WIKI_REMOVE_CHARS = re.compile(\"'+|(=+.{2,30})=+|__TOC__|(ファイル:).+|:(en|de|it|fr|es|kr|zh|no|fi):|\\n\", re.UNICODE)\n",
        "WIKI_SPACE_CHARS = re.compile(\"(\\\\s|゙|゚|　)+\", re.UNICODE)\n",
        "EMAIL_PATTERN = re.compile(\"(^[a-zA-Z0-9_.+-]+@[a-zA-z0-9-]+\\.[a-zA-Z0-9-.]+$)\", re.UNICODE)\n",
        "URL_PATTENR = re.compile(\"((ftp|http|https):\\/\\/)?(www.)?(?!.*(ftp|http|https|www.))[a-zA-Z0-9_-]+(\\.[a-zA-Z]+)+((\\/)[\\w#]+)*(\\/\\/[a-zA-Z0-9_]+=\\w+(&[a-zA-Z0-9_]+=\\w+)?$)\", re.UNICODE)\n",
        "WIKI_REMOVE_TOKEN_CHARS = re.compile(\"(\\\\*$|:$|^파일:.+|^;)\", re.UNICODE)\n",
        "MULTILPE_SPACES = re.compile(' +', re.UNICODE)\n",
        "def tokenizer(content, token_min_len = 2, token_max_len = 100, lower=True):\n",
        "    content = re.sub(EMAIL_PATTERN, ' ', content)\n",
        "    content = re.sub(URL_PATTENR, ' ', content)\n",
        "    content = re.sub(WIKI_REMOVE_CHARS, ' ', content)\n",
        "    content = re.sub(WIKI_SPACE_CHARS, ' ', content)\n",
        "    content = re.sub(MULTILPE_SPACES, ' ', content)\n",
        "    tokens = content.replace(\", )\", \"\").split(\" \")\n",
        "    result = []\n",
        "    for token in tokens:\n",
        "        if not token.startswith('_'):\n",
        "            token_candidate = to_unicode(re.sub(WIKI_REMOVE_TOKEN_CHARS, '', token))\n",
        "        else:\n",
        "            token_candidate = \"\"\n",
        "        if len(token_candidate) > 0:\n",
        "            result.append(token_candidate)\n",
        "    return result"
      ],
      "metadata": {
        "id": "BqIw7X2MoUto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 위키백과 최종 전처리\n",
        "\n",
        "content = '/content/drive/MyDrive/Book/Sentence Embedding Using Korean Corpora/data/Mecab-ko-for-Google-Colab/precessed_wiki_ko.txt'\n",
        "tokenizer(content, token_min_len = 2, token_max_len = 100, lower=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETtSjz0hIxDc",
        "outputId": "70342679-ed03-465c-ff56-b7a861a69607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Book/Sentence',\n",
              " 'Embedding',\n",
              " 'Using',\n",
              " 'Korean',\n",
              " 'Corpora/data/Mecab-ko-for-Google-Colab/precessed_wiki_ko.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2 KorQuAD\n",
        "- KorQuAD는 한국어 기계 독해를 위한 데이터셋이다. LG CNS가 구축해 2018년 공개했으며 학습/데브/테스트셋을 모두 포함해 7만 79건에 이르는 데이터이다.\n",
        "\n",
        "- KorQuAD는 구축 전 과정에서 사람들이 개입하여 검증이 철저하게 이루어졌다."
      ],
      "metadata": {
        "id": "eKOnyJ6jMyME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KorQuAD 데이터 셋 다운로드\n",
        "\n",
        "!wget https://raw.githubusercontent.com/nate-parrott/squad/master/data/train-v1.1.json\n",
        "!wget https://raw.githubusercontent.com/nate-parrott/squad/master/data/dev-v1.1.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNta4U6tOLCl",
        "outputId": "be7462b1-ed84-4aab-f2ad-a4fcb22ddf3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-20 04:17:36--  https://raw.githubusercontent.com/nate-parrott/squad/master/data/train-v1.1.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30288272 (29M) [text/plain]\n",
            "Saving to: ‘train-v1.1.json’\n",
            "\n",
            "train-v1.1.json     100%[===================>]  28.88M  64.8MB/s    in 0.4s    \n",
            "\n",
            "2023-03-20 04:17:40 (64.8 MB/s) - ‘train-v1.1.json’ saved [30288272/30288272]\n",
            "\n",
            "--2023-03-20 04:17:40--  https://raw.githubusercontent.com/nate-parrott/squad/master/data/dev-v1.1.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4854279 (4.6M) [text/plain]\n",
            "Saving to: ‘dev-v1.1.json’\n",
            "\n",
            "dev-v1.1.json       100%[===================>]   4.63M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-03-20 04:17:41 (34.9 MB/s) - ‘dev-v1.1.json’ saved [4854279/4854279]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-8 KorQuAD 전처리 (학습용)\n",
        "\n",
        "import json\n",
        "\n",
        "corpus_fname = '/content/drive/MyDrive/Book/Sentence Embedding Using Korean Corpora/data/Mecab-ko-for-Google-Colab/KorQuAD_v1.0_train.json'\n",
        "output_fname = '/content/drive/MyDrive/Book/Sentence Embedding Using Korean Corpora/data/Mecab-ko-for-Google-Colab/precessed_korquad_train.txt'\n",
        "\n",
        "with open(corpus_fname) as f1, open(output_fname, 'w', encoding = 'utf-8') as f2:\n",
        "    dataset_json = json.load(f1)\n",
        "    dataset = dataset_json['data']\n",
        "    for article in dataset:\n",
        "        w_lines = []\n",
        "        for paragraph in article['paragraphs']:\n",
        "            w_lines.append(paragraph['context'])\n",
        "            for qa in paragraph['qas']:\n",
        "                q_text = qa['question']\n",
        "                for a in qa['answers']:\n",
        "                    a_text = a['text']\n",
        "                    w_lines.append(q_text + \" \" + a_text)\n",
        "        for line in w_lines:\n",
        "            f2.writelines(line + \"\\n\")"
      ],
      "metadata": {
        "id": "XtaZ02kgPbFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-8 KorQuAD 전처리 (데브용)\n",
        "\n",
        "import json\n",
        "\n",
        "corpus_fname = '/content/drive/MyDrive/Book/Sentence Embedding Using Korean Corpora/data/Mecab-ko-for-Google-Colab/KorQuAD_v1.0_dev.json'\n",
        "output_fname = '/content/drive/MyDrive/Book/Sentence Embedding Using Korean Corpora/data/Mecab-ko-for-Google-Colab/precessed_korquad_dev.txt'\n",
        "\n",
        "with open(corpus_fname) as f1, open(output_fname, 'w', encoding = 'utf-8') as f2:\n",
        "    dataset_json = json.load(f1)\n",
        "    dataset = dataset_json['data']\n",
        "    for article in dataset:\n",
        "        w_lines = []\n",
        "        for paragraph in article['paragraphs']:\n",
        "            w_lines.append(paragraph['context'])\n",
        "            for qa in paragraph['qas']:\n",
        "                q_text = qa['question']\n",
        "                for a in qa['answers']:\n",
        "                    a_text = a['text']\n",
        "                    w_lines.append(q_text + \" \" + a_text)\n",
        "        for line in w_lines:\n",
        "            f2.writelines(line + \"\\n\")"
      ],
      "metadata": {
        "id": "j830JP_zQzps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. 네이버 영화 리뷰 말뭉치\n",
        "- 네이버 영화 리뷰 말뭉치는 영화 리뷰들을 평점과 함께 수록한 한국어 말뭉치이다. 감성 분석이나 문서 분류 태스크 수행에 제격이다. 레코드 하나는 문서(리뷰)에 대응한다. 문서 ID, 문서 내용, 레이블(긍정1, 부정0)로 구성돼 있으며 각 열은 탭 문자로 구분돼있다."
      ],
      "metadata": {
        "id": "69NUkiAzk5cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 네이버 영화 리뷰 말뭉치 다운로드\n",
        "\n",
        "# 필요한 라이브러리 및 데이터 불러오기\n",
        "import pandas as pd\n",
        "#import konlpy\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
        "# df = pd.read_table('ratings.txt')\n",
        "# df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd8ykiFdliz5",
        "outputId": "7e024898-a13d-4e39-d152-89e16df53335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ratings.txt', <http.client.HTTPMessage at 0x7fd6fbba0a30>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-11 네이버 영화 리뷰 전처리 코드\n",
        "\n",
        "corpus_path = '/content/drive/MyDrive/Book/Sentence Embedding Using Korean Corpora/data/Mecab-ko-for-Google-Colab/ratings.txt'\n",
        "output_fname = '/content/drive/MyDrive/Book/Sentence Embedding Using Korean Corpora/data/Mecab-ko-for-Google-Colab/process_ratings.txt'\n",
        "with_label = False\n",
        "\n",
        "with open(corpus_path, 'r', encoding = 'utf-8') as f1, open(output_fname, 'w', encoding = 'utf-8') as f2:\n",
        "    next(f1) # 헤드라인 스킵\n",
        "    for line in f1:\n",
        "        _, sentence, label = line.strip().split('\\t')\n",
        "        if not sentence: continue\n",
        "        if with_label:\n",
        "            f2.writelines(sentence + '\\u241E' + label + '\\n')\n",
        "        else:\n",
        "            f2.writelines(sentence + '\\n')"
      ],
      "metadata": {
        "id": "VFVhGXxtmOr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 지도 학습 기반 형태소 분석\n",
        "- 품질 좋은 임베딩을 만들기 위해서는 문장이나 단어의 경계를 컴퓨터에 알려줘야 한다. 그렇지 않으면 어휘 집합에 속한 단어 수가 기하급수적으로 늘어나서 연산의 비효율이 발생한다. 특히 한국어는 조사와 어미가 발달한 교착어이기 때문에 이런 처리를 더 꼼꼼하게 해야한다.\n",
        "\n",
        "- 예를 들어 동사 '가다'는 '가겠다', '가더라'라는 활용이 가능하다. 이런 활용형을 모두 어휘 집합에 넣을 수 있지만 새로운 활용형이 발생할 때마다 계속 늘려야 한다는 단점이 존재한다. 이런 문제를 해소하고자 형태소 분석 기법을 사용한다.\n",
        "\n",
        "- 오픈소스인 Mecab을 사용하여 형태소 분석기로 사용할 수 있다. Mecab을 사용하용 활용형 어휘를 분석하면 아래와 같다.\n",
        "\n",
        "- 가겠다 > 가, 겠, 다\n",
        "- 가더라 > 가, 더라\n",
        "\n",
        "- 이런 형태소 분석 덕분에 어휘 집합 추가 없이 처리할 수 있는 것이다.\n",
        "\n",
        "- 교착어인 한국어는 한정된 종류의 조사와 어미를 자주 이용하기 때문에 각각에 대응하는 명사, 용언(형용사, 동사) 어간만 어휘 집합에 추가하면 취급 단어 개수를 꽤 줄일 수 있다. 형태소 분석기를 잘 사용하면 NLP의 효율성을 높일 수 있다는 것이다.\n",
        "\n",
        "- 이 파트에서는 지도 학습 기반의 형태소 분석을 수행한다. 지도 학습이란 정답이 주어진 데이터를 모델에 넣어 패턴을 학습하게하는 방법이다. 또한 이 파트에서 설명되는 형태소 분석기들은 언어학 전문가들이 태깅한 형태소 분석 말뭉치로부터 학습된 지도 학습 기반 모델들이다.\n",
        "\n",
        "- 태깅이란 아래처럼 모델 입력과 출력 쌍을 만드는 작업을 가리킨다. 이 모델들은 문자열이 주어질 때 사람이 알려준 정답 패턴에 최대한 가깝게 토크나이즈한다.\n",
        "\n",
        "- 입력 : 아버지가방에들어가신다\n",
        "- 출력 : 아버지, 가, 방, 에, 들어가, 신다.\n",
        "\n",
        "### 3.2.1 KoNLPy 사용법\n",
        "- KoNLPy는 Mecab, 꼬꼬마, 한나눔, Okt, 코모란 등 5개 오픈소스 형태소 분석기를 파이썬 화녕에서 사용할 수 있도록한다."
      ],
      "metadata": {
        "id": "4X3DyeJvoDlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KoNLPy 설치\n",
        "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
      ],
      "metadata": {
        "id": "0SOOToCVr0nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-14 Mecab 활용\n",
        "\n",
        "from konlpy.tag import Mecab\n",
        "tokenizer = Mecab()\n",
        "tokenizer.morphs('아버지가방에들어가신다')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCfiDkH0r9Zx",
        "outputId": "dba070d9-82d2-4db8-a338-a5ab8d416694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['아버지', '가', '방', '에', '들어가', '신다']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-15 Mecab 품사 정보 확인\n",
        "tokenizer.pos('아버지가방에들어가신다')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72HWQWsatGtg",
        "outputId": "e0caeb20-ec56-4567-cfe9-c1a718177226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('아버지', 'NNG'),\n",
              " ('가', 'JKS'),\n",
              " ('방', 'NNG'),\n",
              " ('에', 'JKB'),\n",
              " ('들어가', 'VV'),\n",
              " ('신다', 'EP+EC')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-16 get_tokenizer 함수 만들기\n",
        "# 5개 토크나이저 비교를 위함\n",
        "\n",
        "from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n",
        "\n",
        "def get_tokenizer(tokenizer_name):\n",
        "    if tokenizer_name == 'komoran':\n",
        "        tokenizer = Komoran()\n",
        "    elif tokenizer_name == 'okt':\n",
        "        tokenizer = Okt()\n",
        "    elif tokenizer_name == 'mecab':\n",
        "        tokenizer = Mecab()\n",
        "    elif tokenizer_name == 'hannanum':\n",
        "        tokenizer = Hannanum()\n",
        "    elif tokenizer_name == 'kkma':\n",
        "        tokenizer = Kkma()\n",
        "    else:\n",
        "        tokenizer = Mecab()\n",
        "    return tokenizer\n",
        "\n",
        "# 코모란 사용 예시\n",
        "tokenizer = get_tokenizer('komoran')\n",
        "print(tokenizer.morphs('아버지가방에들어가신다'))\n",
        "print(tokenizer.pos('아버지가방에들어가신다'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E81H38oStO_s",
        "outputId": "bbaf8e35-c203-4361-de0a-7cae31c14770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['아버지', '가방', '에', '들어가', '시', 'ㄴ다']\n",
            "[('아버지', 'NNG'), ('가방', 'NNP'), ('에', 'JKB'), ('들어가', 'VV'), ('시', 'EP'), ('ㄴ다', 'EC')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Okt 사용 결과\n",
        "tokenizer = get_tokenizer('okt')\n",
        "print(tokenizer.morphs('아버지가방에들어가신다'))\n",
        "print(tokenizer.pos('아버지가방에들어가신다'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f3-tP6BvPc1",
        "outputId": "905f8ce0-31ef-4369-d161-08d794b920fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['아버지', '가방', '에', '들어가신다']\n",
            "[('아버지', 'Noun'), ('가방', 'Noun'), ('에', 'Josa'), ('들어가신다', 'Verb')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코모란 사용 예시\n",
        "tokenizer = get_tokenizer('komoran')\n",
        "tokenizer.morphs('아버지가방에들어가신다')\n",
        "tokenizer.pos('아버지가방에들어가신다')"
      ],
      "metadata": {
        "id": "dtcFYEjKu1KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.2 KoNLPy 내 분석기별 성능 차이 분석\n",
        "\n",
        "![](https://konlpy-ko.readthedocs.io/ko/v0.4.3/_images/time.png)\n",
        "<center>문자 개수 대비 실행 시간</center>\n",
        "\n",
        "- Mecab이 다른 분석기 대비 상대적으로 속도가 빠르다는 것을 볼 수 있다."
      ],
      "metadata": {
        "id": "5_cenkWhuXa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.3 Khaiii 사용법 (설치문제로 인하여 실습 보류)\n",
        "- Khaiii는 카카오가 2018년 말 공개한 오픈소스 한국어 형태소 분석기이다. 국립국어원이 구축한 세종 코퍼스를 이용해 CNN모델을 적용해 학습했다. Khaiii의 아키텍처 개요는 아래 그림과 같다.\n",
        "\n",
        "![](https://t1.daumcdn.net/thumb/R1280x0/?fname=http://t1.daumcdn.net/brunch/service/user/1oU7/image/DXlTnCNYfeYzWIR4kN428VouYKQ.png)\n",
        "<center>Khaiii 아케텍처</center>\n",
        "\n",
        "- 입력 문장을 문자 단위로 읽고 컨볼루션 필터가 이 문자들을 나눠가면서 정보를 추출한다. 이 네트워크의 말단 레이어에서는 이렇게 모은 정보들을 종합해 형태소의 경계와 품사 태그를 예측한다. GPU없이도 형태소 분석이 가능하며 실행 속도도 빠르다고 한다.\n",
        "\n",
        "### 3.2.4 Mccab에 사용자 사전 추가하기\n",
        "- 형태소 분석기에서 신경 써야할 부분은 토큰들을 어떻게 처리해야 할지다. 예를 들어 '가우스전자'라는 단어에 Mecab을 사용한다고 해보자.\n",
        "\n",
        "- 가우스전자라는 단어가 '가우스', '전자'로 나눠질 것이다. '가우스전자' 하나로 분석됐을 때보다 데이터 분석이나 임베딩 품질이 떨어질 가능서이 발생한다. 따라서 관심 단어들을 가전에 추가해 '가우스전자'같은 단어가 하나의 토큰으로 분석될 수 있도록 강제해야한다.\n",
        "\n",
        "- 다만 Mecab을 colab에서 사용하는 경우 새로운 세션은 지속적으로 설치해야하므로 로컬 환경에서 사용하는 것을 권장한다.\n",
        "\n",
        "[Colab 환경에서 Mecab 사용자 사전 추가 방법](https://somjang.tistory.com/entry/Google-Colab%EC%97%90%EC%84%9C-mecab-ko-dic-%EC%82%AC%EC%9A%A9%EC%9E%90-%EC%82%AC%EC%A0%84-%EC%B6%94%EA%B0%80%ED%95%98%EA%B8%B0)"
      ],
      "metadata": {
        "id": "8r7QpgJnv4cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 비지도 학습 기반 형태소 분석\n",
        "- 이 파트는 비지도 학습 기반의 형태소 분석을 설명한다. 3.2와 달리 비지도 학습 기법들은 데이터의 패턴을 모델 스스로 학습하게 하여 형태소를 분석하는 방법이다. 데이터에 자주 등장하는 단어들을 형태소로 인식한다.\n",
        "\n",
        "### 3.3.1 soynlp 형태소 분석기\n",
        "- soynlp(https://github.com/lovit/soynlp)는 형태소 분석, 품사 판별 등을 지원하는 파이썬 기반 한국어 NLP 패키지이다. 비지도 학습 접근법을 지향하기 때문에 하나의 문장 혹은 문서에서보다는 어느 정도 규모가 있으면서 동질적인 문서 집합에서 잘 작동한다.\n",
        "\n",
        "- soynlp 패키지에 포함된 형태소 분석기는 데이터의 통계량을 확인해 만든 단어 점수 표로 작동한다. 단어 점수는 크게 응집 확률과 브랜칭 엔트로피를 활용한다. 구체적으로 설명하면 주어진 문자열이 유기적으로 연결돼 자주 나타나고(응집 확률이 높고), 그 단어 앞뒤로 다양한 조사, 어미 혹은 다은 단어가 등장하는 경우(브랜칭 엔트로피가 높을 때) 해당 문자열을 형태소로 취급한다.\n",
        "\n",
        "- 예를들어 말뭉치에서 '꿀잼'이라는 단어가 연결돼 자주 나타났다면 '꿀잼'을 형태소라고 보는 것이다(응집 확률이 높다). 한편 꿀잼 앞에 '정말', '너무' 등 문자열이, 뒤에 'ㅋㅋ', 'ㅎㅎ' 등 패턴이 다영하게 나타놨다면 이 역시 '꿀잼'을 형태소로 취급한다(브랜칭 엔트로피가 높다).\n",
        "\n",
        "- soynlp의 형태소 분석기는 우리가 가지고 있는 말뭉치의 통계량을 바탕으로 하기 때문에 별도의 학습 과정이 필요하다. 말뭉치의 분포가 어떻게 돼 있는지 확인하고 단어별로 응집 확률과 브랜칭 엔트로피 점수표를 만드는 절차가 필요하다.\n",
        "\n",
        "- soynlp의 단어 점수를 학습하는 코드는 3-22와 같다."
      ],
      "metadata": {
        "id": "fmyZoA6UCPZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# soynlp 설치 \n",
        "!pip install soynlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfax4YrKGvQt",
        "outputId": "1cfe2f8a-dce5-409b-842d-f14f5edec0c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting soynlp\n",
            "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.8/416.8 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.9/dist-packages (from soynlp) (5.9.4)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.9/dist-packages (from soynlp) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from soynlp) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from soynlp) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.1.0)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-22 soynlp 단어 점수 학습\n",
        "\n",
        "# 라이브러리 불러오기\n",
        "from soynlp.word import WordExtractor\n",
        "\n",
        "# 경로 지정\n",
        "corpus_fname = '/content/drive/MyDrive/Book/Sentence_Embedding_Using_Korean_Corpora/data/process_ratings.txt'\n",
        "model_fname = '/content/drive/MyDrive/Book/Sentence_Embedding_Using_Korean_Corpora/data/soyword.model'\n",
        "\n",
        "# 점수 학습\n",
        "sentences = [sent.strip() for sent in open(corpus_fname, 'r').readlines()] # 입력 문장의 문자 단위로 슬라이싱\n",
        "word_extractor = WordExtractor(min_frequency = 100,\n",
        "                               min_cohesion_forward = 0.05,\n",
        "                               min_right_branching_entropy = 0.0)\n",
        "word_extractor.train(sentences)\n",
        "word_extractor.save(model_fname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVSQ7Mu-CBX1",
        "outputId": "b250e36d-b933-427c-e8b7-f1efa5b54cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training was done. used memory 0.816 Gb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-23 soynlp 형태소 분석\n",
        "\n",
        "# 라이브러리 불러오기\n",
        "import math\n",
        "from soynlp.word import WordExtractor\n",
        "from soynlp.tokenizer import LTokenizer\n",
        "\n",
        "# 학습한 soynlp 모델 불러오기\n",
        "model_fname = '/content/drive/MyDrive/Book/Sentence_Embedding_Using_Korean_Corpora/data/soyword.model'\n",
        "\n",
        "word_extractor = WordExtractor(min_frequency = 100,\n",
        "                               min_cohesion_forward = 0.05,\n",
        "                               min_right_branching_entropy = 0.0)\n",
        "\n",
        "word_extractor.load(model_fname)\n",
        "\n",
        "scores = word_extractor.word_scores()\n",
        "scores = {key:(scores[key].cohesion_forward * math.exp(scores[key].right_branching_entropy)) for key in scores.keys()}\n",
        "tokenizer = LTokenizer(scores = scores)\n",
        "\n",
        "# 토큰 생성\n",
        "tokens = tokenizer.tokenize(\"애비는 종이었다\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFApxGh4I_cW",
        "outputId": "f725bf70-212d-4471-c459-d73ff3fa06b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all cohesion probabilities was computed. # words = 6130\n",
            "all branching entropies was computed # words = 123575\n",
            "all accessor variety was computed # words = 123575\n",
            "['애비는', '종이었다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.2 Google Sentencepice\n",
        "- Sentencepiece는 구글에서 공개한 비지도 학습 기반 형태소 분석 패키지이다. BPE 기법 등을 지원하며 간단하게 python에 설치하여 사용이 가능하다.\n",
        "\n",
        "- BPE(Bite Pair Encoding) 기본 원리는 말뭉치에서 가장 많이 등장하는 문자열을 병합해 문자열을 압축하는 방법이다. 아래 예시를 보자.\n",
        "\n",
        "- aaabdaaabac라는 문자열이 있다. 여기서 'aa' 가장 많이 나타나도 이를 'z'로 치환하면 zabdzabac로 압축할 수 있다. 이 문자열에서 'ab'가 많이 나오니 'y'로 한번 더 압축하면 zydzyac가 될 수 있다.\n",
        "\n",
        "- NLP에서 BPE가 처음 쓰인 태스크는 기계 번역(Machine Translation)이다. BPE를 활용해 토크나이즈하는 방법론의 핵심은 이렇다.\n",
        "\n",
        "- 우선 원하는 어휘 집합의 크기가 될 때까지 반복적으로 고빈도 문자열을 병합해 어휘 집합에 추가한다. 이 방법이 BPE 학습이다.\n",
        "\n",
        "- 학습이 끝난 후 문장 내 각 어절(띄어쓰기로 나눈 것)에 어휘 집합에 있는 서브워드가 포함돼 있을 경우 해당 서브워드를 어절에서 분리한다. 이후 어절의 나머지에서 어휘 집합에 있는 서브워드를 다시 찾고, 또 분리한다. 어절 끝까지 찾았지만 어휘 집합에 없다면 UNK으로 취급한다.\n",
        "\n",
        "- 예를 들어 BPE를 학습한 결과 고빈도 서브워드가 '학교', '법', '먹었' 등이라고 해보자. 그러면 아래 문장은 다음과 같이 분석된다. _로 시작하는 토큰은 해당 토큰이 어절의 시작임을 나타내는 구분자다.\n",
        "\n",
        "- 학교에서 밥을 먹었다 > _학교, 에서, _밥, 을, _먹었, 다\n",
        "\n",
        "- 다음은 전처리를 수행한 한국어 위키백과 데이터를 사용하여 BPE 방법으로 어휘 집합을 만들어본다."
      ],
      "metadata": {
        "id": "HNwTznTxi7hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SentencePiece 설치\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2wownnalM5x",
        "outputId": "f4fef923-abb7-4a55-c0b9-65028e9f0168"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/MyDrive/Book/Sentence_Embedding_Using_Korean_Corpora/data'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7NKnWqVl3Pz",
        "outputId": "d489b71c-aff2-4257-bfe8-e723a05bde8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Book/Sentence_Embedding_Using_Korean_Corpora/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-25 BPE 학습\n",
        "import sentencepiece as spm\n",
        "\n",
        "train = \"\"\"--input = /content/drive/MyDrive/Book/Sentence_Embedding_Using_Korean_Corpora/data/precessed_wiki_ko.txt \\\n",
        "           --model_prefix = sentpiece \\\n",
        "           --vocab_size = 32000 \\\n",
        "           --model_type = bpe --character_coverage = 0.9995\"\"\"\n",
        "\n",
        "spm.SentencePieceTrainer.Train(train)"
      ],
      "metadata": {
        "id": "pDXAUO9IlJap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-25 BPE 학습\n",
        "import sentencepiece as spm\n",
        "\n",
        "corpus = \"/content/drive/MyDrive/Book/Sentence_Embedding_Using_Korean_Corpora/data/precessed_wiki_ko.txt\"\n",
        "prefix = \"sentpiece\"\n",
        "vocab_size = 32000\n",
        "spm.SentencePieceTrainer.train(\n",
        "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
        "    \" --model_type=bpe\")"
      ],
      "metadata": {
        "id": "5FlNAP5pqYxa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mv /content/sentpiece.model /content/drive/MyDrive/Book/Sentence_Embedding_Using_Korean_Corpora/data"
      ],
      "metadata": {
        "id": "aG7dGZVWyoYZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mv /content/sentpiece.vocab /content/drive/MyDrive/Book/Sentence_Embedding_Using_Korean_Corpora/data"
      ],
      "metadata": {
        "id": "HS0lul0UzlUp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BPE로 학습한 어휘 집합을 BERT 모델에도 사용할 수 있다. BPE는 문자열 기반의 비지도 학습 기법이기 때문에 데이터만 확보할 수 있다면 어떤 언어에든 적용이 가능하다.\n",
        "\n",
        "- Sentencepipec 학습 결과를 BERT에 사용하려면 후처리가 필요하다. '_'를 BERT에 맞게 바꾸고 [PAD], [UNK], [CLS], [MASK], [SEP] 등 스페셜 토큰을 추가한다.\n",
        "\n",
        "- "
      ],
      "metadata": {
        "id": "1c48KmJHq9G7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.3 띄어쓰기 교정\n",
        "- soynlp에는 띄어쓰기 교정도 제공한다. 이 기능은 말뭉치에서 띄어쓰기 패턴을 학습한 뒤 해당 패턴대로 교정을 수행한다. 예를 들어 학습 데이터에서 '하자고' 문자열 앞뒤로 공백이 다수 발견됐다면 예측 단계에서 '하자고'가 등장했을 때 앞뒤를 띄어서 교정하는 방식이다.\n",
        "\n",
        "- soynlp의 띄어쓰기 교정 모델 역시 데이터의 통계량을 확인해야 하기 때문에 교정을 수행하기 전 학습이 필요하다.\n",
        "\n",
        "- soynlp 형태소 분석이나 BPE 방식의 토크나이즈 기법은 띄어쓰기에 따라 분석 결과가 크게 달라진다. 따라서 이들 모델을 학습하기 전 띄어쓰기 교정을 먼저 적용하면 그 품석 품질이 개선될 수 있다."
      ],
      "metadata": {
        "id": "Un0c7jbRvKX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soyspacing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UAH2usuz_Yz",
        "outputId": "c02269b8-d6d7-4d1c-db66-094c5b41bc64"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting soyspacing\n",
            "  Downloading soyspacing-1.0.17-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from soyspacing) (1.22.4)\n",
            "Installing collected packages: soyspacing\n",
            "Successfully installed soyspacing-1.0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-28 soynlp 띄어쓰기 모듈 학습\n",
        "# soyspacing 설치하고 다음 코드를 실행하라\n",
        "\n",
        "from soyspacing.countbase import CountSpace\n",
        "\n",
        "corpus_fname = '/content/drive/MyDrive/Book/Sentence_Embedding_Using_Korean_Corpora/data/process_ratings.txt'\n",
        "model_fname = '/content/drive/MyDrive/Book/Sentence_Embedding_Using_Korean_Corpora/data/spcae-correct.model'\n",
        "\n",
        "model = CountSpace()\n",
        "model.train(corpus_fname)\n",
        "model.save_model(model_fname, json_format = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48QxCpxIv0P7",
        "outputId": "f314c7a8-3ab6-4efa-92e4-fb93ba913dd5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all tags length = 1166880 --> 165338, (num_doc = 199991)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드 3-9 soynlp 띄어쓰기 교정\n",
        "from soyspacing.countbase import CountSpace\n",
        "\n",
        "model_fname = '/content/drive/MyDrive/Book/Sentence_Embedding_Using_Korean_Corpora/data/spcae-correct.model'\n",
        "model = CountSpace()\n",
        "model.load_model(model_fname, json_format = False)\n",
        "model.correct(\"어릴때보고 지금다시봐도 재밌어요\")\n",
        "\n",
        "# 띄어쓰기 교정 결과 : 어릴때 보고 지금 다시봐도 재밌어요"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUKLIk8GwunF",
        "outputId": "34952ca1-a8cf-42b3-d456-b62b928333df"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('어릴때 보고 지금 다시봐도 재밌어요', [0, 0, 1, 0, 1, 0, 1, 0, None, 0, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 요약\n",
        "- 임베딩 학습용 말뭉치는 라인 하나가 문서면 좋다. 한국어 위키 백과, KorQuAD, 네이버 영화 말뭉치 등을 이같이 전처리했다.\n",
        "- 지도 학습 기반의 형태소 분석 모델은 언어학 전문가들이 태깅한 형태소 분석 말뭉치로부터 학습된 기법이다. 이 모델들은 문자열이 주어질 때 사람이 정한 정답 패턴에 가깝게 토크나이즈한다. KoNLPy, Khaiii 등이 이 부류에 속한다.\n",
        "- 비지도 학습 기반의 형태소 분석 모델은 데이터의 패턴을 모델 스스로 학습하게 함으로써 형태소를 나누는 기법이다. 데이터에 자주 등장하는 단어들을 형태소로 인식한다. soynlp, Google SentencePicec 등이 이 부류에 속한다."
      ],
      "metadata": {
        "id": "sRgGa4XC2MTh"
      }
    }
  ]
}