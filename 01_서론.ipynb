{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO83I4D9Shgy+Y4kNeIhPh1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgr1118/Sentence-Embedding-Using-Korean-Corpora/blob/main/01_%EC%84%9C%EB%A1%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 서론\n",
        "- 1장에서는 임베딩의 정의와 임베딩이 중요한 이유를 알아보자. 임베딩 기법의 간단한 역사와 종류를 설명한다.\n",
        "\n",
        "## 1.1 임베딩이란\n",
        "- NLP분야에서 임베딩이란, 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 벡터로 바꾼 결과 혹은 그 일련의 과정 전체를 의미한다. 단어나 문장 각각을 벡터로 변환해 벡터 공간으로 넣는다는 의미에서 임베딩이라는 이름이 붙는다.\n",
        "\n",
        "- 우리가 생각하는 가장 간단한 형태의 임베딩은 단어의 빈도를 그대로 벡터로 사용하는 것이다. 아래는 소설들의 단어별 빈도표의 일부이다.\n",
        "\n",
        "표 1-1\n",
        "\n",
        "|구분|메밀꽃 필 무렵|운수 좋은 날|사랑 손님과 어머니|삼포 가는 길|\n",
        "|-|-|-|-|-|\n",
        "|기차|0|2|10|7|\n",
        "|막걸리|0|1|0|0|\n",
        "|선술집|0|1|0|0|\n",
        "\n",
        "- 표 1-1을 보면 운수 좋은 날이라는 문서의 임베딩은 [2,1,1]이다. 막걸리라는 간어의 임베딩은 [0,1,0,0]이다. 이 표를 자세히보면 사랑 손님, 삼포 가는 길은 사용하는 단어 목록이 상대적으로 많이 겹치고 있는 것을 알 수 있다. 이를 바탕으로 우리는 사랑 손님은 삼포 가는 길과 기차라는 소재를 공유한다는 점에서 비슷한 작품일 것이라는 추측을 할 수 있다. 또 막걸리라는 단어와 선술집이라는 단어가 운수 좋은 날이라는 작품에만 등장하는 것을 알 수 있다. 이를 바탕으로 막걸리 - 선술집 간 의미 차이가 막걸리 - 기차 보다 작을 것이라고 추정할 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "KcPzAumlHIkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 임베딩의 역할\n",
        "\n",
        "임베딩은 다음 역할을 수행한다.\n",
        "\n",
        "- 단어/문장 간 관련도 계산\n",
        "- 의미적/문법적 정보 함축\n",
        "- 전이 학습\n",
        "\n",
        "### 1.2.1 단어/문장 간 관련도 계산\n",
        "- 표 1-1의 단어 문서 행렬은 단순한 형태의 임베딩이다. 현업에서는 이보다 더 복잡한 형태의 임베딩을 사용한다. 대표적인 것이 2013년 구글 연구 팀이 발표한 Word2Vec이라는 기법이다. 단어들을 벡터로 바꾸는 방법이다. 아래 예시는 한국어 위키백과, KorQuAD, 네이버 영화 리뷰 말뭉치 등을 Mecab으로 형태소 분석을 한 뒤 100차원으로 학습한 Word2Vec 임베딩이다. 희망이라는 단어의 벡터는 수식 1-1과 같다\n",
        "\n",
        "수식 1-1 '희망'의 Word2Vec 임베딩\n",
        "\n",
        "- $[-.0.00209,  \\ldots, -.-9300]$\n",
        "\n",
        "- 수식 1-1의 숫자들은 모두 100개이다. 100차원으로 임베딩을 했기 때문이다. 사람은 이 숫자들만 봐서는 그 의미를 전혀 이해할 수 없다. 컴퓨터가 계산하기 편하게 '희망'이라는 단어를 벡터로 바꿔놓았다. 하지만 단어를 벡터로 임베딩하는 순간 단어 벡터들 사이의 유사도를 계산하는 일이 가능해진다.\n",
        "\n",
        "표 1-2 쿼리 단어들의 코사인 유사도 기준 상위 5개 단어 목록\n",
        "\n",
        "|희망|절망|학교|학생|가족|자동차|\n",
        "|-|-|-|-|-|-|\n",
        "|소망|체념|초등|대학생|아이|승용차|\n",
        "|행복|고뇌|중학교|대학원생|부모|상용차|\n",
        "|희망찬|절망감|고등학교|교직원|고달픈|대형트럭|\n",
        "|열망|번민|중학|학부모|사랑|모터사이클|\n",
        "\n",
        "- 표 1-2는 각 쿼리 단어별로 벡터 간 유사도 측정 기법의 일종인 코사인 유사도 기준 상위 5개를 나열한 것이다.\n",
        "\n",
        "- 표 1-2에서 관찰할 수 있는 것처럼 '희망'과 코사인 유사도가 가장 높은 것은 '소망'이라는 단어다. 마찬간지로 '절망'은 '체념', '자동차'는 '승용차'와 관련성이 높다. 자연어일 때는 불가능했던 코사인 유사도는 계산이 임베딩 덕분에 가능해졌다. 코사인 유사도는 시각화 기법은 아래 예시처럼 표현이 가능하다.\n",
        "\n",
        "![](https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2F0af2ec84-18bd-456d-b299-e6a216565e62%2Fimage.png)\n",
        "<center>코사인 유사도 예시</center>\n",
        "\n",
        "- 임베딩을 수행하면 벡터 공간을 기하하적으로 나타낸 시각화 역시 가능하다. 그림 1-2는 t-SNE라는 차원 축소 기법으로 n차원의 단어 벡터들을 2차원으로 줄여 이를 시각화한 것이다. 관련성이 높은 단어들이 주변에 몰려 있음을 볼 수 있다. \n",
        "\n",
        "그림 1-2\n",
        "![](https://blog.kakaocdn.net/dn/bGzNT2/btqRqaYpSCI/7vSiLunltSL583owx22zf0/img.png)\n",
        "<center>Word2Vec 단어 벡터들의 2차원 시각화</center>\n",
        "\n",
        "### 1.2.2 의미/문법 정보 함축\n",
        "- 임베딩은 벡터인 만큼 사칙연산이 가능하다. 단어 벡터 간 덧셈/뺄셈을 통해 단어들 사이의 의미적, 문법적 관계를 도출 할 수 있다. 구체적으로 첫 번째 단어 벡터 - 두 번째 단어 벡터 + 세 번째 단어 벡터를 계산해보는 것이다. 그림 1-3처럼 아들 - 딸 + 소녀 = 소년이 성립하면 성공적인 임베딩이라고 볼 수 있다.\n",
        "\n",
        "- 다시말하면 아들 - 딸 사이의 관계와 소년 - 소녀 사이의 의미 차이가 임베딩에 함축돼 있으면 품질이 좋은 임베딩이라 할 수 있다. 이렇게 단어 임베딩을 평가하는 방법을 단어 유추 평가라고 부른다\n",
        "\n",
        "그림 1-3\n",
        "![](https://hyeonukdev.github.io/images/%ED%95%9C%EA%B5%AD%EC%96%B4%EC%9E%84%EB%B2%A0%EB%94%A9/img1_3.png)\n",
        "<center>단어 유추 평가</center>\n",
        "\n",
        "- 표 1-3은 한국어 위키백과, KorQuAD, 네이버 영화 리뷰 말뭉치 등 공개된 데이터로 학습한 Wopd2Vec 임베딩에 단어 유추 평가를 수행한 결과다. 단어1 - 단어2 + 단어3 연산을 수행한 벡터와 코사인 유사도가 가장 높은 단어들이 네 번째 열의 단어들이다.\n",
        "\n",
        "표 1-3\n",
        "\n",
        "|단어1|단어2|단어3|결과|\n",
        "|-|-|-|-|\n",
        "|남동생|여동생|소년|소녀|\n",
        "|신랑|신부|왕|여왕|\n",
        "\n",
        "### 1.2.3 전이 학습\n",
        "- 임베딩은 딥러닝 모델의 입력값으로 쓰인다. 품질 좋은 임베딩은 모델의 성능과 학습 속도를 올려준다. 이렇게 임베딩을 다른 딥러닝 모델의 입력값으로 쓰는 기법을 전이 학습이라고 한다.\n",
        "\n",
        "- 사람이 학습을 할 때 원점부터 시작하지 않듯이 전이 학습도 이렇게 진행된다. 대규모 말뭉치를 활용해 임베딩을 미리 만들어 둔다. 임베딩에는 의미, 문법적 정보 등이 들어있다. 이 임베딩을 입력값으로 쓰는 전이 학습 모델은 태스크를 빠르게 잘 할 수있다.\n",
        "\n",
        "- 교재에서 예시로 양방향 LSTM에 어텐션 매커니즘을 적용하였다. 이 딥러닝 모델의 입력값은 단어 임베딩 기법의 일종인 FastText 임베딩을 사용했다. FastText는 Word2Vec의 개선된 버젼이다. 이 모델의 작동 방식을 설명하면 이렇다. 다음과 같은 학습 데이터가 있다고 하자.\n",
        "\n",
        "- 이 영화 꿀잼 + 긍정\n",
        "- 이 영화 노잼 + 부정\n",
        "\n",
        "- 이 전이 학습 모델은 문장을 입력받으면 해당 문장의 긍/부정을 출력한다. 문장을 형태소 분석한 뒤 각각의 형태소에 해당하는 FastText 단어 임베딩이 모델의 입력값이 된다. 수식 1-2의 이, 영화, 꿀잼 임베딩 세개가 순차적으로 입력된다. 두 번째 문장은 이, 영화, 노잼 임베딩 세 개가 차례로 들어간다. 이후 모델은 이 입력 벡터들을 계산해 문장의 극성을 예츠한다.\n",
        "\n",
        "수식 1-2\n",
        "\n",
        "- $e_이 = [-0.20195 \\ldots -0.0674]$\n",
        "- $e_영화 = [-0.07806 \\ldots -0.11161]$\n",
        "- $e_꿀잼 = [-0.08889 \\ldots -0.02243]$\n",
        "- $e_노잼 = [-0.00956 \\ldots -0.01423]$\n",
        "\n",
        "- 그림 1-4는 모델의 학습 과정별 단순 정확도를 나타낸 것이다. 학습 데이터는 네이버 영화 리뷰를 사용했다. 이 데이터는 문장과 극성이 쌍으로 정리돼 있어 문서 극성 분류 모델의 학습 데이터로 좋다.\n",
        "\n",
        "- 그림 1-4의 파란색 선은 FastText는 모델 입력값으로 FastText 임베딩을 사용한 것이고, 보라색 선 Random은 임력 벡터를 랜덤 초기화한 뒤 모델을 학습한 방식이다. 다시 말해 학습을 원점부터 시작한 모델이라는 것이다. 그림에서 FastText 임베딩을 사용한 결과 더 좋게 나오는 것을 볼 수 있다.\n",
        "\n",
        "그림 1-4\n",
        "![](https://i.imgur.com/Dd4e8u2.png)\n",
        "<center>임베딩 종류별 긍/부정 문서 분류 정확도</center>\n",
        "\n",
        "- 그림 1-5는 문서 극성 분류 모델의 학습 과정별 학습 손실을 그래프로 나타낸 것이다. FastText 임베딩을 사용한 모델의 학습 손실이 Random보다 적고 모델의 수렴이 빠르다는 것을 알 수 있다. 품질 좋은 임베딩을 사용해야하는 이유이다.\n",
        "\n",
        "그림 1-4\n",
        "![]https://i.imgur.com/3Yi6FjB.png)\n",
        "<center>임베딩 종류별 학습 손실</center>"
      ],
      "metadata": {
        "id": "NhPGo-yrWjES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 임베딩 기업의 역사와 종류\n",
        "임베딩 기법의 발전 흐름과 종류를 간단하게 알아본다.\n",
        "\n",
        "### 1.3.1 통계 기반에서 뉴럴 네트워크 기반으로\n",
        "- 초기 임베딩 기법은 대부분 말뭉치의 통계량을 직접적으로 활용하였다. 대표적인 기법이 잠재 의미 분석(LSA)이다. LSA란 단어 사용 빈도 등 말뭉치의 통계량 정보가 드렁 있는 커다란 행렬에 특이값 분해 등 수학적 기법을 적용해 행렬에 속한 벡터들의 차원을 축소하는 방법을 말한다. 아래 예시가 있다.\n",
        "\n",
        "표 1-6\n",
        "\n",
        "||문서1|문서2|문서3|문서4|\n",
        "|-|-|-|-|-|\n",
        "|단어1|2|0|0|0|\n",
        "|단어2|0|1|0|0|\n",
        "|단어3|0|0|0|3|\n",
        "|단어4|1|0|1|2|\n",
        "\n",
        "표 1-6-1\n",
        "\n",
        "||주제1|주제2|\n",
        "|-|-|-|\n",
        "|단어1|0.42|1.92|\n",
        "|단어2|1.03|-0.29|\n",
        "|단어3|2.88|-0.69|\n",
        "|단어4|2.29|0.64|\n",
        "\n",
        "표 1-6-2\n",
        "\n",
        "||문서1|문서2|문서3|문서4|\n",
        "|-|-|-|-|-|\n",
        "|주제1|0.81|0.27|0.59|3.70|\n",
        "|주제2|2.08|-0.13|0.30|-0.49|\n",
        "\n",
        "- 단어-문서 행렬에 LSA를 적용했다고 가정해보자. 단어-문서 행렬은 보통 행의 개수가 매우 많다. 말뭉치 전체의 어휘 수와 같기 때문이다. 어휘 수는 대개 10~20만 개 내외에 달한다. 게다가 행렬 대부분의 요소 값은 0으로 구성된 희소 행렬이다.\n",
        "\n",
        "- 문제는 이런 희소 행렬을 다른 모델의 입력값으로 사용하면 계산량과 메모리가 많이 소비된다. 따라서 원래 행렬의 차원을 축소해 사용한다. 표 1-6-1처럼 단어를 기준으로, 표 1-6-2처럼 문서를 기준으로 줄일 수 있다. 결과적으로 전자는 단어 수준 임베딩, 후자는 문서 임베딩이 된다.\n",
        "\n",
        "- LSA 수행 대상 행렬은 여러 종류가 될 수 있다. 단어-문서 행렬을 비롯해 TF-IDF, 단어 문맥 행렬, 점별 상호 정보화 행렬 등이 있다.\n",
        "\n",
        "- 최근에는 뉴럴 네트워크 기반의 임베딩 기법들이 주목받고 있다. Neural Probabilistic Language Model(Bengio et al., 2003)이 발표된 이후이다. 뉴럴 네트워크 기반 모델들은 이전 단어들이 주어졌을 때 다음 단어가 뭐가 될지 예측하거나, 문장 내 일부분에 마스킹을 하고 해당 단어가 무엇인지 맞추는 과정에서 학습된다. 예시는 교재 38p 참고\n",
        "\n",
        "### 1.3.2 단어 수준에서 문장 수준으로\n",
        "- 2017년 이전의 임베딩 기법들은 대게 단어 수준 모델이었다. NPLM, Word2Vec, GloVe, FastText, Swivel 등이 있다. 단어 임베딩은 각각의 벡터에 해당 단어의 문맥적 의미를 함축한다. 하지만 단어 수준 임베딩 기법의 단점은 동의이의어를 분간하기 어렵다는 것이다. 단어의 형태가 같다면 동일한 단어로 보고, 모든 문맥 정보를 해당 단어 벡터에 투영하기 때문이다.\n",
        "\n",
        "- 2018년 초에 ElMo가 발표된 이후 문장 수중 임베딩 기법들이 주목을 받았다. BERT와 GPT 등이 여기 속한다. 문장 수준 임베딩은 개별 단어가 아닌 단어 시퀀스 전체의 문맥적 의미를 함축하기 때문에 단어 임베딩 기법보다 전이 학습에 유리하다.\n",
        "\n",
        "### 1.3.3 룰 > 엔드투엔드 > 프리트레인/파인튜닝\n",
        "- 과거에는 NLP 모델 대부분은 사람이 직접 feature를 뽑았다. 한국어 구문 분석하는 모델을 만든다고 할 때, 피처 추출은 언어학적인 지식을 활용한다. 언어학적인 규칙(rule)을 모델에게 알려주는 것이다.\n",
        "\n",
        "- 2000년대 중반 이후 NLP에서도 딥러닝 모델이 주목받기 시작했다. 모델은 입력과 출력사이의 관계를 잘 근사하기 때문에 사람이 굳이 규칙을 알려주지 않아도 된다.이런 기법을 엔드투엔드라고하고 하며 기계 번역에 널리쓰이는 Seq2Seq 모델이 엔드투엔드의 대표 사례다.\n",
        "\n",
        "- 2018년 ELMo 모델이 제안된 이후 NLP 모델은 엔드투엔드에서 프리트레인/파인 튜닝 방식으로 발전하고 있다. 먼저 대규모 말뭉치로 임베딩을 만든다(프리트레인). 이 임베딩에는 말뭉치의 의미, 문법적 맥락이 포함돼 있다. 이후 임베딩을 입력으로 하는 새로운 딥러닝 모델을 만들고 우리가 풀고싶은 구체적 문제에 맞는 소규모 데이터에 맞게 임베딩을 포함한 모델 전체를 업데이트한다(파인 튜닝, 잔이 학습).\n",
        "\n",
        "- 우리가 풀고 싶은 자연어 처리의 구체적 문제들은 다운스트림 태스크라고한다. 품사 판별, 개체명 인식, 의미역 분석 등이 여기에 속한다. 이에 대비되는 개념이 업스트림 태스크이다. 다운스트림에 앞서 해결해야 할 과제라는 뜻이다. 단어/문장 임베딩을 프리트레인하는 작업이 바로 업스트림 태스크이다.\n",
        "\n",
        "- NLP에서 업스트림과 다운스트림 태스크 사이는 연계되있다. 품질이 좋은 임베딩이 문제를 제대로 해소할 수 있다는 것이다. 다음스트림 태스크를 예시와 함께 살펴본다.\n",
        "\n",
        "- 품사 판별 : 나는 네가 지난 여름에 한 [일]을 알고 있다. > 일:명사\n",
        "- 문장 성분 분석 : 나는 [네가 지난 여름에 한 일]을 알고 있다. > 네가 지난 여름에 한 일 : 명사구\n",
        "- 의존 관계 분석 : [자연어 처리는] 늘 그렇듯이 [재미있다] > 자연어 처리는, 재미있다 : 주격명사구\n",
        "- 의미역 분석 : 나는 [네가 지난 여름에 한 일]을 알고 있다. > 네가 지난 여름에 한 일 : 피행위주역\n",
        "- 상호 참조 해결 : 나는 어제 [기락이]를 만났다. [그]는 셔츠를 입고 있었다. > 그 = 기락이\n",
        "\n",
        "### 1.3.4 임베딩의 종류와 성능\n",
        "이 교재에ㅓ 다루는 임베딩 기법은 크게 3가지로 나뉜다. 행렬 분해, 예측, 토픽 기반\n",
        "\n",
        "행렬 분해 기반 방법\n",
        "- 행렬 분해 기반 방법은 말뭉치 정보가 들어 있는 원래 행렬을 두 개 이상의 작은 행렬로 쪼개는 방식이다. 분해한 이후 둘 중 하난의 행렬만 쓰거나 둘을 더하거나 이어 붙여 임베딩으로 사용한다. GloVe, Swivel 등이 바로 여기에 속한다\n",
        "\n",
        "그림 1-9 \n",
        "\n",
        "![](https://minkithub.github.io/img/recommendation/post2/MF2.png)\n",
        "<center>행렬 분해 기반 방법</center>\n",
        "\n",
        "예측 기반 방법\n",
        "- 이 방법은 어떤 단어 주변에 특정 단어가 나타날지 예측하거나, 이전 단어들이 주어졌을 때 다음 단어가 무엇인지 예측하거나, 문장 내 일부 단어를 지우고 해당 단어가 무엇인지 맞추는 과정에서 학습하는 방법이다. 뉴럴 네트워크 방법들이 예측 기반 방법에 속한다. Word2Vec, FastText, BERT, GPT, ELMo 등이 있다.\n",
        "\n",
        "토픽 기반 방법\n",
        "- 주어진 문서에 잠재된 주제를 추론하는 방식으로 임베딩을 수행한다. 잠재 디레클리 할당(LDA)이 대표적인 기법이다. LDA같은 모델은 학습이 완료되면 각 문서가 어떤 주제 분포를 갖는지 확률 벡터 형태로 반환하기 때문에 임베딩 기법의 일종으로 이해할 수 있다.\n",
        "\n",
        "임베딩 성능 평가\n",
        "- 영어 기반 다운스트림 태스크에 대한 임베딩 종류별 성능을 측정하기 위해 형태소 분석, 문장 성분 분석, 의존 관계 분석, 의미역 분석, 상호 참조 해결 등으로 평가하였다. 파인 튜닝 모델의 구조를 고정한 뒤 각각의 임베딩을 전이 학습을시켜 정확도를 측정했다. 결과는 문장 임베딩 기법인 ELMo, BERT, GPT가 단어 임베딩 기법인 GloVe를 앞섰다."
      ],
      "metadata": {
        "id": "8YCgAmuiWpBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 개발 환경\n",
        "\n",
        "### 1.4.1 도커 환경 구성하기\n",
        "- 이 교재의 소스 코드 실행을 위해 도커 환경을 구성해야 한다. 도커란 컨테이너 기반의 오픈소스 가상화 플랫폼이다. 도커가 있으면 컴퓨팅 환경이 어떤 것이든 관계없이 컨테이너에서 제공하는 환경을 그대로 사용할 수 있다. 다시 말해 나의 운영체제가 윈도우든 리눅스든 상관없이 우분투 환경에 최적화된 이 교재의 코드를 실행할 수 있는 것이다.\n",
        "\n",
        "- [우분투 및  설치 참고](https://kk-7790.tistory.com/125)\n",
        "- [우분투 에러 해결](https://lucidmaj7.tistory.com/324)"
      ],
      "metadata": {
        "id": "rDtvEtN7mktf"
      }
    }
  ]
}