{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2r3xmD4xeFdB//vZS55wr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgr1118/Sentence-Embedding-Using-Korean-Corpora/blob/main/02_%EB%B2%A1%ED%84%B0%EA%B0%80_%EC%96%B4%EB%96%BB%EA%B2%8C_%EC%9D%98%EB%AF%B8%EB%A5%BC_%EA%B0%80%EC%A7%80%EA%B2%8C_%EB%90%98%EB%8A%94%EA%B0%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 벡터가 어떻게 의미를 가지게 되는가\n",
        "- 2장에서는 자연어의 의미를 임베딩에 어떻게 녹여낼 수 있는지 알아본다.\n",
        "\n",
        "## 2.1 자연어 계산과 이해\n",
        "- 컴퓨터는 자연어를 사람처럼 이해할 수 없기 때문에 임베딩을 통하여 계산한다. 임베딩은 자연어를 컴퓨터가 처리할 수 있는 숫자들의 나열인 벡터로 바꾼 결과이기 때문이다.\n",
        "\n",
        "- 임베딩에 자연어 의미를 함축하는 비법은 자연어의 통계적 패턴 정보를 통째로 임베딩에 넣는 것이다. 자연어의 의미는 해당 언어 화자들이 실제 사용하는 일상 언어에서 드러나기 때문이다. 임베딩을 만들 때 쓰는 통계 정보는 아래 3가지 있다.\n",
        "\n",
        "표 2-1 임베딩을 만드는 세 가지 철학\n",
        "\n",
        "|구분|백오브워즈|언어 모델|분포 가정|\n",
        "|-|-|-|-|\n",
        "|내용|어떤 단어가 (많이) 쓰였는가|단어가 어떤 순서로 쓰였는가|어떤 단어가 같이 쓰였는가|\n",
        "|대표 통계량|TF-IDF|-|PMI|\n",
        "|대표 모델|Deep Averaging Network|ELMo, GPT|Word2Vec|\n",
        "\n",
        "- 백오브워즈는 어떤 단어가 (많이) 쓰였는지 정보를 중시한다. 저자의 의도는 단어 사용 여부나 그 빈도에서 드러난다고 보기 때문이다. 단어의 순서 정보는 무시한다. 이 방법에서 가장 많이 쓰이는 통계량은 TF-IDF이며 딥러닝 버젼은 Deep Averaging Network이다.\n",
        "\n",
        "- 단어의 등장 순서를 무시하는 백오브워즈의 대척점은 언어 모델이 있다. 언어 모델은 단어의 등장 순서를 학습해 주어진 단어 시퀀스가 얼마나 자연스러운지 확률을 부여한다. ELMo, GPT 등과 같은 뉴럴 네트워크 기반의 언어 모델이 여기에 해당한다.\n",
        "\n",
        "- 분포 가정에서는 문장에서 어떤 단어가 같이 쓰였는지를 중요하게 본다. 단어의 의미는 그 주변 문맥을 통해 유츄해볼 수 있다고 보는 것이다. 분포 가정의 대표 통계량은 점별 상호 정보량이며 대표적인 모델은 Word2Vec이다.\n",
        "\n",
        "- 이 세 가지 철학은 서로 연관이있다. 언어 모델에서는 단어의 등장 순서를 , 분포 가정에서는 문맥을 우선시한다. 어떤 단어가 문장에서 주로 나타나는 순서는 해당 단어의 주변 문맥과 연관되기 때문이다. 한편 분포 가정에서는 어떤 단어 쌍(pair)이 얼마나 같이 자주 나타나는지와 관련한 정보를 수치화하기 위해 개별 단어 그리고 단어 쌍의 빈도 정보를 적극 활용한다.\n",
        "\n",
        "- 백오브워즈, 언어 모델, 분포 가정은 말뭉치의 통계적 패턴을 서로 다른 각도에서 분석하는 것이며 상호보완적이다.\n",
        "\n"
      ],
      "metadata": {
        "id": "KcPzAumlHIkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 어떤 단어가 많이 쓰였는가\n",
        "\n",
        "### 2.2.1 백오브워즈 가정\n",
        "- 수학에서 백(bag)이란 중복 원소를 허용한 집합을 뜻한다. 원소의 순서는 고려하지 않는다. 예컨대 중복집합 {a,a,b}는 {a,b,a}, {b,a,a}와 같다.\n",
        "\n",
        "- NLP에서 백오브워즈란 단어의 등장 순서에 관계없이 문서 내 단어의 등장 빈도를 임베딩으로 쓰는 기법을 말한다. 이 기법은 문장을 단어들로 나누고 이들을 중복집합에 넣어 임베딩으로 활용하는 것이다. \n",
        "\n",
        "![](https://i.ibb.co/fkqh1Yt/bow01.jpg)\n",
        "<center>백오브워즈 예시 1</center>\n",
        "\n",
        "![](https://vitalflux.com/wp-content/uploads/2021/08/Bag-of-words-technique-to-convert-to-numerical-feature-vector-png.png)\n",
        "<center>백오브워즈 예시 2</center>\n",
        "\n",
        "- 백오브워즈 임베딩에서는 주제가 비슷한 문서라면 단어 빈도 또는 단어 등장 여부 역시 비슷할 것이고, 백오브워즈 임베딩 역시 유사할 것이라고 보는 것이다. 빈도를 그대로 백오브워즈로 쓴다면 많이 쓰인 단어가 주제와 더 강한 관련을 맺고 있을 것이라는 전제가 바탕이다.\n",
        "\n",
        "표 2-2 단어-문서 행렬\n",
        "\n",
        "|구분|메밀꽃 필 무렵|운수 좋은 날|사랑 손님과 어머니|삼포 가는 길|\n",
        "|-|-|-|-|-|\n",
        "|기차|0|2|10|7|\n",
        "|막걸리|0|1|0|0|\n",
        "|선술집|0|1|0|0|\n",
        "\n",
        "- 표 2-2는 1장에서 본 단어-문서 행렬이다. 백오브워즈 임베딩은 간단한 기법이지만 정보 검색 분야에서 여전히 많이 쓰인다. 사용자의 질의(query)에 가장 적절한 문장을 보여줄 때 질의를 백오브워즈 임베딩으로 변환하고 질의에 검색 대상과 대상 문서 임베딩 간 코사인 유사도를 구해 유사도가 가장 높은 문서를 사용자에게 보여준다.\n",
        "\n",
        "### 2.2.2 TF-IDF\n",
        "- 백오브워즈에는 큰 단점이 있다. 어떤 문서에든 쓰여서 해당 단어가 (많이) 나타났다 하더라도 문서의 주제를 가늠하기 어려운 경우가 있기 때문이다. 예를 들어 '을/를', '이/가' 같은 조사는 대부분의 한국어 문서에 등장한다. 이에 이런 조사만으로는 해당 문서의 주제를 추측하기 어렵다.\n",
        "\n",
        "- 이런 단점을 보완하기 위해 제안된 기법이 TF-IDF이다. 단어-문서 행렬에 수식 2-1과 같이 가중치를 계해 행렬 원소를 바꾼다. 이 방법 역시 단어의 등장 순서를 고려하지 않는다는 점에서 백오브워즈 임베딩이라고 할 수 있다. \n",
        "\n",
        "수식 2-1\n",
        "- $TF - IDF(w) = TF(w) * log\\frac{N}{DF(w)}$\n",
        "\n",
        "- 수식 2-1에서 TF(Term Frequency)는 어떤 단어가 특정 문서에 많이 쓰였는지 빈도를 나타낸다. 많이 쓰인 단어가 중요하다는 가정을 전제로 한 수치이다. \n",
        "\n",
        "- DF(Document Frequency)는 특정 단어가 나타난 문서의 수를 뜻한다. DF가 클수록 다수 문서에 쓰이는 범용적인 단어라고 볼 수 있겠다. TF는 같은 단어라도 문서마다 다른 값을 가지고 DF는 문서가 달라도 단어가 같다면 동일한 값을 가진다.\n",
        "\n",
        "- IDF(Inverse Document Frequency)는 전체 문서 수를 해당 단어의 DF로 나눈 뒤 로그를 취한 값이다. 그 값이 클수록 특이한 단어라는 뜻이다. 이는 단어의 주제 예측 능력(해단 단어만 보고 문서의 주제를 가늠해볼 수 있는 정도)과 연결된다.\n",
        "\n",
        "- TF-IDF가 지향하는 원리는 다음과 같다. 어떤 단어의 주제 예측 능력이 강할 수록 가중치가 커지고 그 반대의 경우 작아진다. 표 2-3을 보면 '어머니'라는 명사는 사랑 손님과 어머니라는 문서에 의존명사 '것'보다 TF-IDF 값이 크다. '것'만 봐서는 해당 문서의 주제를 예측하기 힘들지만, '어머니'가 나왔다면 문서 주제를 상대적으로 예측하기 수월하다. 한편 TF가 높아도 TF-IDF 값 역시 커진다. 단어 사용 빈도는 저자가 성정한 주제와 관련을 맺고 있을 것이라는 가정에 기초한 것이다.\n",
        "\n",
        "표 2-3 TF-IDF\n",
        "\n",
        "|구분|메밀꽃 필 무렵|운수 좋은 날|사랑 손님과 어머니|삼포 가는 길|\n",
        "|-|-|-|-|-|\n",
        "|어머니|0.066|0.0|0.595|0.0|\n",
        "|것|0.2622|0.098|0.145|0.0848|\n",
        "\n",
        "### 2.2.3 Deep Averaging Network\n",
        "- Deep Averaging Network는 백오브워즈 가정의 뉴럴 네트워크 버젼이다.\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbOo1sW%2FbtqDOk1YlQW%2FT8C8hvz6ARorKp9RTeIRf0%2Fimg.png)\n",
        "<center>Deep Averaging Network</center>\n",
        "\n",
        "- Deep Averaging Network는 문장의 임베딩은 중복집합 임베딩을 평균을 취해 만든다.\n",
        "\n",
        "- Deep Averaging Network는 문장 내에 어떤 단어가 쓰였는지, 쓰였다면 얼마나 많이 쓰였는지 그 빈도만을 따진다. Deep Averaging Network는 이러한 문장 임베딩을 입력 받아 해당 문서가 어떤 범주인지 분류한다. 간단하지만 성능이 좋기에 현업에서도 자주 쓰인다.\n",
        "\n"
      ],
      "metadata": {
        "id": "NhPGo-yrWjES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 단어가 어떤 순서로 쓰였는가\n",
        "\n",
        "### 2.3.1 통계 기반 언어 모델\n",
        "- 언어 모델이란 단어 시퀀스에 확률을 부여하는 모델이다. 단어의 등장 순서를 무시하는 백오브워즈와 달리 시퀀스 정보를 명시적으로 학습한다.\n",
        "\n",
        "- 단어가 n개 주어진 상황이라면 언어 모델은 n개 단어가 동시에 나타날 확률 $P(w_1 ⋯ w_n)$을 반환한다. 잘 학습된 언어 모델이 있다면 어떤 문자잉 그럴듯한지, 주어진 단어 시퀀스 다음 단어는 무엇이 올 때 자연스러운지 알 수 있다. 아래는 예시이다.\n",
        "\n",
        "- 누명을 쓰다 > 0.41\n",
        "- 누명을 당하다 > 0.02\n",
        "\n",
        "- n-gram이란 n개 단어를 뜻하는 용어이다. '난폭, 운전' 같은 경우는 2-gram 또는 바이그램(bigram)이다. 세 개의 단어가 묶이면 3-gram 또는 트라이그램(trigram)이라 한다. 경우에 따라서 n-gram은 n-gram에 기반한 언어 모델을 의미하기도 한다. 말뭉치 내 단어들은 n개씩 묶어서 그 빈도를 학습했다는 뜻이다.\n",
        "\n",
        "- 표 2-4는 네이버 영화 리뷰 말뭉치에서 각각의 표현이 등장한 횟수를 나타낸다. 띄어쓰기 단위인 어절을 하나의 단어로 보고 빈도를 센 것이다.\n",
        "\n",
        "표 2-4 네이버 영화 말뭉치의 각 표현별 등장 횟수\n",
        "\n",
        "|빈도|문서1|\n",
        "|-|-|\n",
        "|내|1309|\n",
        "|마음|172|\n",
        "|최고의|3503|\n",
        "|최고의 명작이다|23|\n",
        "|내 마음 속에 영원히 기억될 최고의 명작이다|0|\n",
        "\n",
        "- 표 2-4를 보면 '내 마음 속에 영원히 기억될 최고의 명작이다'는 이 말뭉치에 한번도 등장하지 않는다. 이런 말뭉치로 학습한 언어 모델은 해당 표현이 나타날 확률을 0으로 본다. 즉, 문법적으로 의미적으로 결합이 없지만 해당 표현이 말이 되지 않는 문장으로 취급된다는 것이다.\n",
        "\n",
        "- '내 마음 속에 영원히 기억될 최고의'라는 표현 다음에 '명작이다'라는 단어가 나타날 확률은 조건부확률의 정의를 활용해 MLE로 유도하면 수식 2-2와 같다. \n",
        "\n",
        "수식 2-2 '내 마음 속에 영원히 기억될 최고의' 다음에 '명작이다'가 나올 확률\n",
        "- $P(명작이다|내, 마음, 속에, 영원히, 기억될, 최고의) = \\frac{Freq(내, 마음, 속에, 영원히, 기억될, 최고의, 명작이다)}{Freq(내, 마음, 속에, 영원히, 기억될, 최고의)}$\n",
        "\n",
        "- 하지만 이 식에서 분자가 0이므로 확률은 0이다.\n",
        "\n",
        "- n-gram 모델을 쓰면 이런 문제를 일부 해결할 수 있다. 직전 n-1개 단어의 등장 확률로 전체 단어 시퀀스 등장 확률을 근사한는 것이다. 이는 한 상태의 확률은 그 직전 상태에만 의존한다는 마코프 가정에 기반한 것이다.\n",
        "\n",
        "- '내 마음 속에 영원히 기억될 최고의' 다음에 '명작이다'가 나타날 확률을 바이그램 모델로 근사하면 수식 2-3과 같다. '최고의 명작이다' 빈도를 '최고의' 빈도로 나눠준 것이다. 다시 말하면 '명작이다' 직전의 1개 단어만 보고 전체 단어 시퀀스 등장 확률을 근사한 것이다.\n",
        "\n",
        "수식 2-3 바이그램 근사 예시 (1)\n",
        "- $P(명작이다|내, 마음, 속에, 영원히, 기억될, 최고의) \\approx P(명작이다|최고의)\n",
        "= \\frac{Freq(최고의, 명작이다)}{Freq(최고의)} = \\frac{23}{3503}$\n",
        "\n",
        "- 다음으로 바이그램 모델에서 '내 마음 속에 영원히 기억될 최고의 명작이다'라는 단어 시퀀스가 나올 확률은 수식 2-4와 같다.\n",
        "\n",
        "수식 2-4 바이그램 근사 예시 (2)\n",
        "- $P(명작이다|내, 마음, 속에, 영원히, 기억될, 최고의, 명작이다) \\approx P(내) * P(마음|내) * P(속에|마음) * P(영원히|속에) * P(기억될|영원히) * P(최고의|기억될) * P(명작이다|최고의) = $\n",
        "- $\\frac{1309}{|V|}, \\frac{93}{1309}, \\frac{9}{172}, \\frac{7}{155}, \\frac{7}{104}, \\frac{1}{29}, \\frac{23}{3503}$\n",
        "\n",
        "- 바이그램 일반화 모델을 일반화한 식은 수식 2-5와 같다. n-gram 모델은 바이그램 모델의 확장판으로 직전 1개 단어만 참고하는 바이그램 모델과 달리 전체 시퀀스 등장 확률 계산 시 직전 n-1개 단어의 히스토리를 본다.\n",
        "\n",
        "수식 2-5 바이그램 모델\n",
        "- $P(w_n|w_n-1) = \\frac{Freq(w_n-1, w_n)}{Freq(w_n-1)}$\n",
        "- $P(w_1^n) = P(w_1, w_2, ⋯, w_n) = \\prod_{k=1}^n P(w_k|w_k-1)$\n",
        "\n",
        "- 하지만 문제는 여전히 남아 있다. 데이터에 한 번도 등장하지 않는 n-gram이 존재할 때 예측 단계에서 문제가 발생할 수 있다. 아래는 예시이다.\n",
        "\n",
        "- $P(그 아이는 또바기 인사를 잘한다) = P(그) * P(아이는|그) * P(또바기|아이는) = 0 * P(인사를|또바기) * P(잘한다|인사를) = 0$\n",
        "\n",
        "- 이를 위해 다음과 같은 방식이 제안됐다.\n",
        "- 백오프 : n-gram 등장 빈도를 n보다 작은 범위의 단어 시퀀스 빈도로 근사, n을 크게하면할수록 등장하지 않는 케이스가 많아질 가능성이 높기 때문이다. 예를 들어 7-gram 모델을 적용하면 '내 마음 속에 영원히 기억될 최고의 명작이다'의 등장 빈도는 0이다.\n",
        "\n",
        "- 이를 백오프 방식(N을 4로 줄임)으로 7-gram 빈도를 근사하면 수식 2-6과 같다. 수식 2-5에서 $\\alpha, \\beta$는 실제 빈도와의 차이를 보정해주는 파라미터이다. 물론 빈도가 1 이상인 7-gram에 대해서는 백오프하지 않고 해당 빈도를 그대로 n-gram 모델 학습에 사용한다.\n",
        "\n",
        "수식 2-6 백오프 기법 예시\n",
        "- $Freq(내 마음 속에 영원히 기억될 최고의 명작이다) ≈ \\alpha Freq(영원히 기억될 최고의 명작이다) + \\beta$\n",
        "\n",
        "- 스무딩은 표 2-4와 같은 등장 빈도 표에 모두 k만큼 더하는 기법이다. 이렇게 된다면 빈도가 0이었던 단어에 +k만큼 빈도가 추가된다. 이 때문에 Add-k 스무딩이라고 부른다.\n",
        "\n",
        "- 스무딩을 시행하면 높은 빈도를 가진 문자열 등장 확률을 일부 깎고 학습 데이터에 전혀 등장하지 않는 케이스들에는 작으나마 일부 확률을 부여하게 된다.\n",
        "\n",
        "### 2.3.2 뉴럴 네트워크 기반 언어 모델\n",
        "- 앞서 설명한 모델은 단어들의 빈도를 세어 학습한다. 이것을 뉴럴 네트워크로 학습할 수도 있다. 뉴럴 네트워크는 입력과 출력 사이의 관계를 유연하게 포착해낼 수 있고, 그 자체로 확률 모델로 기능할 수 있기 때문이다.\n",
        "\n",
        "입력 : 발 업는 말이 > 언어 모델 > 출력 : 천리\n",
        "\n",
        "- 위 예시처럼 주어진 단어 시퀀스를 가지고 다음 단어를 맞추는 과정에서 학습한다. 학습이 완료되면 이들 모델의 중간 혹은 끝 계산 결과물을 단어나 문장의 임베딩으로 활용하고 ELMo, GPT등 모델이 여기에 해당한다.\n",
        "\n",
        "- 마스크 언어 모델(MLM)은 언어 모델 기반 방법과 큰 틀에서 유사하지만 조금 다르다. 아래 예시처럼 문장 중간에 마스크를 씌우고 해당 위치에 어떤 단어가 올지 예측하는 과정에서 학습한다.\n",
        "\n",
        "입력 : 발 없는 말이 [MASK] 간다 > 언어 모델 > 출력 : 천리\n",
        "\n",
        "- 언어 모델은 순차적으로 단어를 입력받아 일방향이지만 MLM은 문장 전체를 보고 중간에 있는 단어를 예측하기 때문에 양방향 학습이 가능하다. BERT가 여기에 속한다."
      ],
      "metadata": {
        "id": "8YCgAmuiWpBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 어떤 단어가 같이 쓰였는가\n",
        "\n",
        "### 2.4.1 분포 가정\n",
        "- NLP에서 분포란 특정 범위, 즉 윈도우내에 동시에 등장하는 이웃 단어 또는 문맥의 집합을 말한다.어떤 단어 쌍이 비슷한 문맥 환경에 자주 등장한다면 그 의미 또한 유사할 것이라는 것이 분포 가정의 전제다.\n",
        "\n",
        "- 분포 가정은 모국어 화자들이 해당 단어를 실제 어떻게 사용하고 있는지 문맥을 살핌으로써 그 단어의 의미를 밝힐 수 있다는 이야기다.\n",
        "\n",
        "- 예를 들어 '빨래', '세탁'이라는 단어의 의미를 모든가도 가정하자. 의미를 파악하기 위해서는 이들 단어들이 실제로 어떻게 쓰이는지 보면된다. 여기서 '빨래', '세탁'은 각각 타깃 언어, '청소', '물' 등은 문맥 단어이다.\n",
        "\n",
        "- 특기 는 자칭 청소 와 빨래 지만 요리 는 절망 적 ...\n",
        "- 세탁, 청소, 요리 와 가사 는...\n",
        "\n",
        "- '빨래'와 '세탁'의 문맥 단어가 유사하여 이 두 단어는 유사한 의미를 지닐 것으로 예상할 수 있다. 그러나 개별 단어의 분포 정보와 그 의미 사이에는 논리적으로 직접적인 연관성이 있어 보이지는 않는다. 다시 말해 분포 정보가 곧 의미라는 분포 가정에 의문이 발생한다. 다음에는 둘 사이에 어떤 관계가 있는지 언어학적 관점에서 알아본다.\n",
        "\n",
        "### 2.4.2 분포와 의미 (1) : 형태소\n",
        "- 언어학에서 형태소란 의미를 가지는 최소 단위를 말한다. '철수가 밥을 먹었다.'라는 예시에서 '철수'를 '철', '수'로 나눠버리면 의미가 사라진다. 이런 점에서 '철수'는 형태소라 할 수 있다.\n",
        "\n",
        "- 언어학자별 형태소를 분석하는 방법은 조금 다르다. 대표적인 기준은 계열 관계가 있다. 계열 관계는 형태소 자리에 다른 형태소가 '대치'돼 쓰일 수 있는가를 따지는 것이다. \n",
        "\n",
        "- 위 예시 문장에서 '철수'를 '기락'으로 바꿔도 같은 의미가 된다. 이를 근거로 '철수'에 형태소 자격을 부여한다. 이는 언어학자등리 특정 타깃 단어 주변의 문맥 정보를 바탕으로 형태소를 확인한다는 것이다. 말뭉치의 분포 정보와 형태소가 밀저반 관계를 이루고 있다는 것이다.\n",
        "\n",
        "### 2.4.3 분포와 의미 (2) : 품사\n",
        "\n"
      ],
      "metadata": {
        "id": "KZwATZmM5iRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 개발 환경\n",
        "\n",
        "### 1.4.1 도커 환경 구성하기\n",
        "- 이 교재의 소스 코드 실행을 위해 도커 환경을 구성해야 한다. 도커란 컨테이너 기반의 오픈소스 가상화 플랫폼이다. 도커가 있으면 컴퓨팅 환경이 어떤 것이든 관계없이 컨테이너에서 제공하는 환경을 그대로 사용할 수 있다. 다시 말해 나의 운영체제가 윈도우든 리눅스든 상관없이 우분투 환경에 최적화된 이 교재의 코드를 실행할 수 있는 것이다.\n",
        "\n",
        "- [우분투 및  설치 참고](https://kk-7790.tistory.com/125)\n",
        "- [우분투 에러 해결](https://lucidmaj7.tistory.com/324)"
      ],
      "metadata": {
        "id": "rDtvEtN7mktf"
      }
    }
  ]
}